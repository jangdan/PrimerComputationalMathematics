---
redirect_from:
  - "/maths/big-o-notation"
interact_link: notebooks/maths/Big_O_notation.ipynb
kernel_name: python3
kernel_path: notebooks/maths
has_widgets: false
title: |-
  Big O notation
pagenum: 2
prev_page:
  url: /maths/intro.html
next_page:
  url: /maths/Taylor_series.html
suffix: .ipynb
search: x h f y frac o delta toc mathcal notation t class span limit e example computational point term c small case error ldots rightarrow right n href data modified id item mathematical explanation computer science iv terms errors order behaviour where le considering size quad text quadratically introduction numnbspnbsp series infinite complexity en wikipedia org wiki very such algorithms above v constant left its enough larger factor problem g asymptotic cost div itemlispana lilispana li ul big taylor xh hf talking away convergence run times means well simply here magnitude replaced bounded relative large measure lefty infty things values

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /notebooks***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Big O notation</div>
</div>
    
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><h1>Table of Contents<span class="tocSkip"></span></h1></p>
<div class="toc"><ul class="toc-item"><li><span><a href="#Introduction-and-notation" data-toc-modified-id="Introduction-and-notation-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Introduction and notation</a></span></li><li><span><a href="#Mathematical-explanation" data-toc-modified-id="Mathematical-explanation-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Mathematical explanation</a></span><ul class="toc-item"><li><span><a href="#Example" data-toc-modified-id="Example-2.1"><span class="toc-item-num">2.1&nbsp;&nbsp;</span>Example</a></span></li></ul></li><li><span><a href="#Use-in-Computer/Computational-Science" data-toc-modified-id="Use-in-Computer/Computational-Science-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>Use in Computer/Computational Science</a></span></li></ul></div>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Big-O-notation">Big O notation<a class="anchor-link" href="#Big-O-notation"> </a></h1><h2 id="Introduction-and-notation">Introduction and notation<a class="anchor-link" href="#Introduction-and-notation"> </a></h2><p>In the notebook on Taylor series we arrived at the infinite Taylor series <em>about (or around) the point $x_0$</em>:</p>
$$f(x) = f(x_0) + (x - x_0) f'(x_0) + \frac{(x - x_0)^2}{2!} f''(x_0) + \frac{(x - x_0)^3}{3!} f'''(x_0) +
\frac{(x - x_0)^4}{4!} f^{(iv)}(x_0) + \ldots$$<p>and we pointed out that an equivalent way of writing this expansion is</p>
$$ f(x_0+h)  = f(x_0) + hf'(x_0) + \frac{h^2}{2!}f''(x_0) + \frac{h^3}{3!}f'''(x_0) + \ldots $$<p>or replace $h$ by the notation $\Delta x$ or $\delta x$.</p>
<p>When talking about terms in infinite series (terms we will often be forces to truncate, i.e. throw away), or talking about errors, convergence, complexity, run times etc, so-called <a href="https://en.wikipedia.org/wiki/Big_O_notation">Big-O</a> is very useful.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What this means is that mathematically we have some notation that allows us to write these infinite expansions in forms such as</p>
$$f(x) = f(x_0) + (x - x_0) f'(x_0) + \frac{(x - x_0)^2}{2!} f''(x_0) + \frac{(x - x_0)^3}{3!} f'''(x_0) + \mathcal{O}((x - x_0)^4)$$<p>or</p>
$$ f(x_0+h) = f(x_0) + hf'(x_0) + \frac{h^2}{2!}f''(x_0) + \frac{h^3}{3!}f'''(x_0) + \mathcal{O}(h^4) $$<p>But what does this mean?</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Mathematical-explanation">Mathematical explanation<a class="anchor-link" href="#Mathematical-explanation"> </a></h2><p>Well simply $\mathcal{O}$ here means "order".</p>
<p>More formally it is used to signify (place bounds on) the limiting behaviour of the magnitude of a mathematical term (or an algorithm's runtime).</p>
<p>So based on the the example above we have simply replaced the terms</p>
$$ \frac{h^4}{4!} f^{(iv)}(x_0) +  \frac{h^5}{5!} f^{(v)}(x_0) + \ldots $$<p>with</p>
$$ \mathcal{O}(h^4) $$<p>"in the limit" (and implicitly for this use application we are interested in the limit as $h\rightarrow 0$)</p>
<p>i.e. we have written that</p>
$$ \frac{h^4}{4!} f^{(iv)}(x_0) +  \frac{h^5}{5!} f^{(v)}(x_0) + \ldots  =  \mathcal{O}(h^4) $$<p>this is stating that as $h$ tends to zero the LHS can bounded in magnitude by a term of the form</p>
$$ C h^4 $$<p>where $C$ is a constant, i.e. there exists a constant $C$ such that for all sufficiently small $h$</p>
$$\left| \frac{h^4}{4!} f^{(iv)}(x_0) +  \frac{h^5}{5!} f^{(v)}(x_0) + \ldots \right| \le C h^4 $$<p>In our case the point is that, assuming $f$ is a well-behaved function, i.e. it's derivatives are bounded, then there will be a small enough $h$ such that the terms dependent on the 5th and higher powers or $h$ are very small relative to the first term</p>
<p>They can't be ignored, but we can select a $C$ which is a bit larger than original factor ${h^4}/{4!}$ so that the above holds.</p>
<p>The point here isn't to actually find $C$, this is just notation to help convey a point.</p>
<p>It's important when analysing errors in algorithms.</p>
<p>Note that for this case we were considering the limit of small $h$ and so the lowest power eventually becomes dominant, the opposite situation would occur if we were considering the limit of large $h$.</p>
<p>With errors we are generally thinking about the former case, with run-times (where $h$ might be replaced with some measure of the size of the problem) we are in the latter case.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Example">Example<a class="anchor-link" href="#Example"> </a></h3><p>Consider</p>
$$g(y) = 3y^2 - y^3 + 9y^4$$<p>in the limit as $y\rightarrow 0$.</p>
<p>We can show that</p>
$$\left| 3y^2 - y^3 + 9y^4 \right| \le \left| 3y^2\right| + \left|y^3\right| + \left|9y^4 \right| \le 3y^2 + y^2 + 9y^2 \le 13 y^2$$<p>as we are considering the case as $y$ gets small. Hence we can write</p>
$$g(y) = 3y^2 - y^3 + 9y^4 = \mathcal{O}(y^2) \quad\text{as}\quad y\rightarrow 0$$<p>If we were considering the case of $y\rightarrow \infty$, then we would write instead</p>
$$g(y) = \mathcal{O}(y^4) \quad\text{as}\quad y\rightarrow \infty$$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When dealing with numerical errors you may see things like</p>
$$\text{error} \approx 10^{-5}\Delta t +  10^{5}\Delta t^2$$<p>For larger values of $\Delta t$ the second term will clearly dominate due to the relative size of the constant in front of it compared to the first term.</p>
<p>But when performing a convergence analysis, i.e. investigating how the error drops as $\Delta t$ is reduced, there will come a point for small enough $\Delta t$ that the first term starts to dominate, and from the point onwards the error will decay linearly rather than quadratically, i.e. halving $\Delta t$ leads to a reduction in the error by a factor of 2 rather than 4.</p>
<p>So we would say that</p>
$$\text{error} = \mathcal{O}(\Delta t)$$<p>even though at larger $\Delta t$ values the error would be observed to decay quadratically as we reduce $\Delta t$.</p>
<p>We use the expression "in the asymptotic limit" to refer to region of parameter space where the leading order behaviour dominates. For the above example if we observe something that looks like second order behaviour (when we expect first) we would explain this away by saying that we are not in the asymptotic limit. If we see very close to first order behaviour then we would say we are in the asymptotic limit.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Use-in-Computer/Computational-Science">Use in Computer/Computational Science<a class="anchor-link" href="#Use-in-Computer/Computational-Science"> </a></h2><p>For the cost of algorithms we use the notation in a similar manner.</p>
<p>An algorithm is said to have (<em>time</em> or <em>algorithmic</em> or <em>computational</em>) <a href="https://en.wikipedia.org/wiki/Time_complexity">complexity</a> of $\mathcal{O}(n^2)$ for example,
if for large enough $n$, where $n$ is a measure of the problem size, the computational cost grows quadratically - for every doubling of the problem size the cost grows by a factor 4.</p>
<p>When it comes to complexity it's quite common to see things like $\mathcal{O}(n \log n)$, i.e. the algorithm scales worse than linearly, but not as bad as quadratically.</p>
<p>For some examples see <a href="https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations">https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations</a></p>

</div>
</div>
</div>
</div>

 


    </main>
    